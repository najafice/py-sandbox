{
<<<<<<< HEAD
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06776ced",
   "metadata": {},
   "source": [
    "# Regular Expressions (ReGex)\n",
    "\n",
    "## Project 1: Finding correct e-mail address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4417b2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "email = input('Enter your e-mail address: ')\n",
    "valid_mail = r\"(\\w+)@(\\w+).(\\w{2,3})\"\n",
    "\n",
    "if re.search(valid_mail, email) == None:\n",
    "    print(\"Not OK!\") \n",
    "else:\n",
    "    print('OK') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbeccfde",
   "metadata": {},
   "source": [
    "## Project 2: Scraping Divar.ir\n",
    "برنامه‌ای بنوسید که آگهی‌های دارای تگ نردبان در صفحه اول سایت دیوار را استخراج و در خروجی چاپ کند."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e22ce251",
   "metadata": {},
   "outputs": [
=======
  "cells": [
    {
      "cell_type": "markdown",
      "id": "06776ced",
      "metadata": {
        "id": "06776ced"
      },
      "source": [
        "# Regular Expressions (ReGex)\n",
        "\n",
        "## Project 1: Finding correct e-mail address"
      ]
    },
>>>>>>> bee0c56 (Push from Colab :: adding third essay.)
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4417b2ac",
      "metadata": {
        "id": "4417b2ac",
        "outputId": "2a18b752-a0ff-46ed-b326-11ba57b3770f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "email = input('Enter your e-mail address: ')\n",
        "valid_mail = r\"(\\w+)@(\\w+).(\\w{2,3})\"\n",
        "\n",
        "if re.search(valid_mail, email) == None:\n",
        "    print(\"Not OK!\")\n",
        "else:\n",
        "    print('OK')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbeccfde",
      "metadata": {
        "id": "cbeccfde"
      },
      "source": [
        "## Project 2: Scraping Divar.ir\n",
        "برنامه‌ای بنوسید که آگهی‌های دارای تگ نردبان در صفحه اول سایت دیوار را استخراج و در خروجی چاپ کند."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e22ce251",
      "metadata": {
        "id": "e22ce251",
        "outputId": "8101f2af-a816-433b-b169-807bccec32ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "نردبان فلزی >>>> کارکرده >>>> ۸۰۰,۰۰۰ تومان\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "divar_url = 'https://divar.ir/s/isfahan/toolbox?q=%D9%86%D8%B1%D8%AF%D8%A8%D8%A7%D9%86'\n",
        "\n",
        "resp = requests.get(divar_url)\n",
        "if resp.status_code == 200:\n",
        "    soup = BeautifulSoup(resp.text, 'html.parser')\n",
        "\n",
        "    quotes = []\n",
        "    divs = soup.find_all('div', class_='post-list__items-container-e44b2')\n",
        "\n",
        "    for box in divs:\n",
        "        try:\n",
        "            title = box.find('h2').text.strip()\n",
        "            meta_data = box.find_all('div', class_='kt-post-card__description')\n",
        "\n",
        "            description = meta_data[0].text.strip() if len(meta_data) > 0 else ''\n",
        "            price = meta_data[1].text.strip() if len(meta_data) > 1 else ''\n",
        "\n",
        "            quote = {\n",
        "                'title': title,\n",
        "                'description': description,\n",
        "                'price': price\n",
        "            }\n",
        "            quotes.append(quote)\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing box: {e}\")\n",
        "\n",
        "    # Display extracted quotes\n",
        "    for q in quotes:\n",
        "        print(f'{q['title']} >>>> {q['description']} >>>> {q['price']}')\n",
        "else:\n",
        "    print('Cannot access the Divar.ir website!')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project 3: Scraping [scrapethissite.com](https://www.scrapethissite.com/pages/simple/)\n",
        "برنامه ای بنویسید که از سایت بالا نام کشور، پایتخت، جمعیت و مساحت 20 کشور اول را بگیرد و در یک دیتابیس دلخواه ذخیره کند."
      ],
      "metadata": {
        "id": "9N1q2cAvxk2F"
      },
      "id": "9N1q2cAvxk2F"
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import sqlite3\n",
        "from bs4 import BeautifulSoup\n",
        "from tabulate import tabulate  # pip install tabulate\n",
        "\n",
        "# Importing the HTML data from the URL\n",
        "scrape_url = 'https://www.scrapethissite.com/pages/simple/'\n",
        "res = requests.get(scrape_url)\n",
        "\n",
        "if res.status_code == 200:\n",
        "    soup = BeautifulSoup(res.text, 'html.parser')\n",
        "\n",
        "    data = []\n",
        "    divs = soup.find_all('div', class_='col-md-4 country')\n",
        "    for div in divs:\n",
        "        try:\n",
        "            country_name = div.find('h3', class_='country-name').text.strip()\n",
        "            country_capital = div.find('span', class_='country-capital').text.strip()\n",
        "            country_popu = div.find('span', class_='country-population').text.strip()\n",
        "            country_area = div.find('span', class_='country-area').text.strip()\n",
        "\n",
        "            country_dict = {\n",
        "                'Name': country_name,\n",
        "                'Capital': country_capital,\n",
        "                'Population': int(float(country_popu)),\n",
        "                'Area': int(float(country_area))\n",
        "            }\n",
        "            data.append(country_dict)\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing box: {e}\")\n",
        "\n",
        "    # Connect to SQLite database\n",
        "    conn = sqlite3.connect('countries.db')\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    # Create table\n",
        "    cursor.execute('''\n",
        "        CREATE TABLE IF NOT EXISTS countries (\n",
        "            Name TEXT,\n",
        "            Capital TEXT,\n",
        "            Population INTEGER,\n",
        "            Area REAL\n",
        "        )\n",
        "    ''')\n",
        "\n",
        "    # Clear old data to avoid duplicates\n",
        "    cursor.execute('DELETE FROM countries')\n",
        "\n",
        "    # Insert data rows\n",
        "    for row in data:\n",
        "        cursor.execute('''\n",
        "            INSERT INTO countries (Name, Capital, Population, Area)\n",
        "            VALUES (?, ?, ?, ?)\n",
        "        ''', (row['Name'], row['Capital'], row['Population'], row['Area']))\n",
        "\n",
        "    conn.commit()\n",
        "\n",
        "    # Query top 10 countries by population\n",
        "    cursor.execute('''\n",
        "        SELECT Name, Capital, Population, Area\n",
        "        FROM countries\n",
        "        ORDER BY Population DESC\n",
        "        LIMIT 10\n",
        "    ''')\n",
        "    top_countries = cursor.fetchall()\n",
        "    conn.close()\n",
        "\n",
        "    # Format numbers with commas\n",
        "    formatted_data = [\n",
        "        (name, capital, f\"{population:,.0f}\", f\"{area:,.0f}\")\n",
        "        for name, capital, population, area in top_countries\n",
        "    ]\n",
        "\n",
        "    headers = [\"Name\", \"Capital\", \"Population\", \"Area (km²)\"]\n",
        "    print(tabulate(formatted_data, headers=headers, tablefmt=\"fancy_grid\", stralign=\"center\", numalign=\"right\"))\n",
        "else:\n",
        "    print('Could not receive any data!')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oy1Mf9epx4z-",
        "outputId": "2b7635c9-b990-40d8-857b-f56316f46c37"
      },
      "id": "oy1Mf9epx4z-",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "╒═══════════════╤════════════╤═══════════════╤══════════════╕\n",
            "│     Name      │  Capital   │  Population   │  Area (km²)  │\n",
            "╞═══════════════╪════════════╪═══════════════╪══════════════╡\n",
            "│     China     │  Beijing   │ 1,330,044,000 │  9,596,960   │\n",
            "├───────────────┼────────────┼───────────────┼──────────────┤\n",
            "│     India     │ New Delhi  │ 1,173,108,018 │  3,287,590   │\n",
            "├───────────────┼────────────┼───────────────┼──────────────┤\n",
            "│ United States │ Washington │  310,232,863  │  9,629,091   │\n",
            "├───────────────┼────────────┼───────────────┼──────────────┤\n",
            "│   Indonesia   │  Jakarta   │  242,968,342  │  1,919,440   │\n",
            "├───────────────┼────────────┼───────────────┼──────────────┤\n",
            "│    Brazil     │  Brasília  │  201,103,330  │  8,511,965   │\n",
            "├───────────────┼────────────┼───────────────┼──────────────┤\n",
            "│   Pakistan    │ Islamabad  │  184,404,791  │   803,940    │\n",
            "├───────────────┼────────────┼───────────────┼──────────────┤\n",
            "│  Bangladesh   │   Dhaka    │  156,118,464  │   144,000    │\n",
            "├───────────────┼────────────┼───────────────┼──────────────┤\n",
            "│    Nigeria    │   Abuja    │  154,000,000  │   923,768    │\n",
            "├───────────────┼────────────┼───────────────┼──────────────┤\n",
            "│    Russia     │   Moscow   │  140,702,000  │  17,100,000  │\n",
            "├───────────────┼────────────┼───────────────┼──────────────┤\n",
            "│     Japan     │   Tokyo    │  127,288,000  │   377,835    │\n",
            "╘═══════════════╧════════════╧═══════════════╧══════════════╛\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    },
    "colab": {
      "provenance": []
    }
<<<<<<< HEAD
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "divar_url = 'https://divar.ir/s/isfahan/toolbox?q=%D9%86%D8%B1%D8%AF%D8%A8%D8%A7%D9%86'\n",
    "\n",
    "resp = requests.get(divar_url)\n",
    "if resp.status_code == 200:\n",
    "    soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "\n",
    "    quotes = []\n",
    "    divs = soup.find_all('div', class_='post-list__items-container-e44b2')\n",
    "\n",
    "    for box in divs:\n",
    "        try:\n",
    "            title = box.find('h2').text.strip()\n",
    "            meta_data = box.find_all('div', class_='kt-post-card__description')\n",
    "\n",
    "            description = meta_data[0].text.strip() if len(meta_data) > 0 else ''\n",
    "            price = meta_data[1].text.strip() if len(meta_data) > 1 else ''\n",
    "\n",
    "            quote = {\n",
    "                'title': title,\n",
    "                'description': description,\n",
    "                'price': price\n",
    "            }\n",
    "            quotes.append(quote)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing box: {e}\")\n",
    "\n",
    "    # Display extracted quotes\n",
    "    for q in quotes:\n",
    "        print(f'{q['title']} >>>> {q['description']} >>>> {q['price']}')\n",
    "else:\n",
    "    print('Cannot access the Divar.ir website!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff6ee6a",
   "metadata": {},
   "source": [
    "## Project 3: Scraping [scrapethissite.com](https://www.scrapethissite.com/pages/simple/)\n",
    "برنامه ای بنویسید که از سایت Scrapethissite نام کشور،پایتخت، جمعیت و مساحت 20 کشور اول را بگیرد و در یک دیتابیس دلخواه ذخیره کند."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461cfc3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
=======
>>>>>>> bee0c56 (Push from Colab :: adding third essay.)
  },
  "nbformat": 4,
  "nbformat_minor": 5
}